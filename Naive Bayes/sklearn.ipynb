{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d09271",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# import libraries\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "\n",
    "# import libraries\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,precision_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f982012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the amazon review dataset\n",
    "df = pd.read_csv('../data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6b5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e60a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text:str):\n",
    "    \n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    stemmer =  PorterStemmer()\n",
    "    \n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    tokens_2 = [stemmer.stem(token) for token in filtered_tokens if token not in stopwords_set]\n",
    "\n",
    "    return tokens_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53f5145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_counts(reviews):\n",
    "    word_count = defaultdict(int)\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = preprocess_text(review)\n",
    "        \n",
    "        for token in tokens:\n",
    "            word_count[token] += 1\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c3e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df,test_size=0.30, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab59ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'film': 5,\n",
       "             'notabl': 1,\n",
       "             'shown': 4,\n",
       "             'much': 2,\n",
       "             'incred': 1,\n",
       "             'poverti': 1,\n",
       "             'sicili': 1,\n",
       "             '19th': 1,\n",
       "             'centuri': 1,\n",
       "             'gave': 2,\n",
       "             'way': 1,\n",
       "             '20th': 1,\n",
       "             'life': 2,\n",
       "             'style': 1,\n",
       "             'made': 2,\n",
       "             'peopl': 5,\n",
       "             'dream': 1,\n",
       "             'new': 3,\n",
       "             'world': 2,\n",
       "             'america': 3,\n",
       "             'mancuso': 2,\n",
       "             'famili': 1,\n",
       "             'live': 1,\n",
       "             'place': 1,\n",
       "             'even': 3,\n",
       "             'hamlet': 1,\n",
       "             'stone': 3,\n",
       "             'cottag': 1,\n",
       "             'set': 2,\n",
       "             'amid': 1,\n",
       "             'harsh': 1,\n",
       "             'unyield': 1,\n",
       "             'countri': 2,\n",
       "             'offer': 2,\n",
       "             'singl': 1,\n",
       "             'blade': 1,\n",
       "             'green': 1,\n",
       "             'grass': 1,\n",
       "             'open': 1,\n",
       "             'sequenc': 1,\n",
       "             'see': 2,\n",
       "             'male': 1,\n",
       "             'scrambl': 1,\n",
       "             'barefoot': 1,\n",
       "             'craggi': 1,\n",
       "             'hillsid': 1,\n",
       "             'mouth': 1,\n",
       "             'shrine': 1,\n",
       "             'top': 1,\n",
       "             'exchang': 1,\n",
       "             'sign': 1,\n",
       "             'remain': 1,\n",
       "             'show': 1,\n",
       "             'aboard': 2,\n",
       "             'liner': 2,\n",
       "             'huddl': 1,\n",
       "             'mass': 1,\n",
       "             'inde': 3,\n",
       "             'yearn': 1,\n",
       "             'breath': 1,\n",
       "             'free': 1,\n",
       "             'condit': 1,\n",
       "             'elli': 2,\n",
       "             'island': 2,\n",
       "             'journey': 1,\n",
       "             'still': 1,\n",
       "             'interrog': 1,\n",
       "             'examin': 1,\n",
       "             'prove': 1,\n",
       "             'fit': 1,\n",
       "             'enter': 1,\n",
       "             'ship': 3,\n",
       "             'longshot': 1,\n",
       "             'shot': 2,\n",
       "             'would': 4,\n",
       "             'identifi': 2,\n",
       "             'larg': 1,\n",
       "             'oceango': 1,\n",
       "             'also': 2,\n",
       "             'anyth': 1,\n",
       "             'clich√©': 1,\n",
       "             'view': 1,\n",
       "             'york': 1,\n",
       "             'harbour': 1,\n",
       "             'statu': 1,\n",
       "             'liberti': 1,\n",
       "             'could': 1,\n",
       "             'anywher': 1,\n",
       "             'perhap': 1,\n",
       "             'remark': 1,\n",
       "             'one': 2,\n",
       "             'camera': 1,\n",
       "             'crane': 1,\n",
       "             'look': 3,\n",
       "             'hundr': 1,\n",
       "             'jam': 1,\n",
       "             'togeth': 1,\n",
       "             'slowli': 1,\n",
       "             'almost': 2,\n",
       "             'impercept': 1,\n",
       "             'two': 3,\n",
       "             'third': 2,\n",
       "             'screen': 2,\n",
       "             'left': 2,\n",
       "             'begin': 1,\n",
       "             'separ': 1,\n",
       "             'right': 2,\n",
       "             'realiz': 1,\n",
       "             'actual': 2,\n",
       "             'dock': 1,\n",
       "             'power': 1,\n",
       "             'statement': 1,\n",
       "             'societi': 1,\n",
       "             'fragment': 1,\n",
       "             'there': 1,\n",
       "             'strong': 1,\n",
       "             'documentari': 1,\n",
       "             'feel': 1,\n",
       "             'throughout': 1,\n",
       "             'though': 2,\n",
       "             'follow': 1,\n",
       "             'full': 1,\n",
       "             'immigr': 1,\n",
       "             'care': 1,\n",
       "             'script': 1,\n",
       "             'clearli': 1,\n",
       "             'amalgam': 1,\n",
       "             'typic': 1,\n",
       "             'familiescondit': 1,\n",
       "             'time': 2,\n",
       "             'noth': 1,\n",
       "             'happen': 1,\n",
       "             'dramat': 1,\n",
       "             'term': 1,\n",
       "             'popcorn': 1,\n",
       "             'brigad': 1,\n",
       "             'multiplex': 1,\n",
       "             'rest': 1,\n",
       "             'us': 1,\n",
       "             'fine': 1,\n",
       "             'absenc': 4,\n",
       "             'good': 2,\n",
       "             'plot': 1,\n",
       "             'decent': 2,\n",
       "             'act': 2,\n",
       "             'cinematographi': 1,\n",
       "             'special': 2,\n",
       "             'effectsne': 1,\n",
       "             'go': 1,\n",
       "             'review': 1,\n",
       "             'may': 1,\n",
       "             'contain': 1,\n",
       "             'spoiler': 1,\n",
       "             'actor': 2,\n",
       "             'appear': 2,\n",
       "             'read': 1,\n",
       "             'line': 2,\n",
       "             'well': 1,\n",
       "             'like': 2,\n",
       "             'second': 2,\n",
       "             'grade': 1,\n",
       "             'play': 1,\n",
       "             'stori': 1,\n",
       "             'written': 1,\n",
       "             'aforement': 1,\n",
       "             'gradersit': 1,\n",
       "             'realli': 1,\n",
       "             'convolutedit': 1,\n",
       "             'simpl': 2,\n",
       "             'dumb': 1,\n",
       "             'person': 1,\n",
       "             'think': 3,\n",
       "             'must': 1,\n",
       "             'miss': 1,\n",
       "             'someth': 1,\n",
       "             'convolut': 1,\n",
       "             'nope': 1,\n",
       "             'exactli': 1,\n",
       "             'understood': 1,\n",
       "             'that': 1,\n",
       "             'know': 1,\n",
       "             'stunk': 1,\n",
       "             'sit': 1,\n",
       "             'around': 1,\n",
       "             'talkingread': 1,\n",
       "             'tri': 1,\n",
       "             'sinist': 1,\n",
       "             'narrat': 1,\n",
       "             'annoy': 1,\n",
       "             'effect': 1,\n",
       "             'laughabl': 1,\n",
       "             'love': 2,\n",
       "             'low': 1,\n",
       "             'budget': 1,\n",
       "             'movi': 2,\n",
       "             'carolyn': 1,\n",
       "             'munro': 1,\n",
       "             'tom': 1,\n",
       "             'savini': 1,\n",
       "             'jack': 1,\n",
       "             'scarri': 1,\n",
       "             'michael': 1,\n",
       "             'berrymorejust': 1,\n",
       "             'moviey': 1,\n",
       "             'tell': 1,\n",
       "             'werent': 2,\n",
       "             'get': 2,\n",
       "             'paid': 3,\n",
       "             'neither': 1,\n",
       "             'heart': 1,\n",
       "             'talent': 1,\n",
       "             'toni': 4,\n",
       "             'toddhowev': 1,\n",
       "             'adequ': 1,\n",
       "             'fact': 1,\n",
       "             'todd': 2,\n",
       "             'perform': 1,\n",
       "             'reason': 1,\n",
       "             '3': 1,\n",
       "             'star': 1,\n",
       "             'instead': 1,\n",
       "             '1and': 1,\n",
       "             'whole': 1,\n",
       "             'minut': 2,\n",
       "             'serious': 1,\n",
       "             'suggest': 1,\n",
       "             'fast': 1,\n",
       "             'forward': 1,\n",
       "             'dvd': 1,\n",
       "             'segment': 1,\n",
       "             'gone': 1,\n",
       "             'theater': 1,\n",
       "             'dollar': 1,\n",
       "             'pod': 1,\n",
       "             'demand': 1,\n",
       "             'money': 1,\n",
       "             'back': 1,\n",
       "             'hope': 1,\n",
       "             'better': 1,\n",
       "             'next': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_word_counts(train['review'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30fd0daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcalculate_word_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreview\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcalculate_word_counts\u001b[39m\u001b[34m(reviews)\u001b[39m\n\u001b[32m      2\u001b[39m word_count = defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     tokens = \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[32m      8\u001b[39m         word_count[token] += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      3\u001b[39m text = text.translate(\u001b[38;5;28mstr\u001b[39m.maketrans(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, string.punctuation))\n\u001b[32m      5\u001b[39m tokens = word_tokenize(text.lower())\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m filtered_tokens = [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m      9\u001b[39m stemmer =  PorterStemmer()\n\u001b[32m     11\u001b[39m stopwords_set = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Mestrado\\Inteligencia_Artificial\\Projeto\\Artificial-Inteligence\\.venv\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[39m, in \u001b[36mWordListCorpusReader.words\u001b[39m\u001b[34m(self, fileids, ignore_lines_startswith)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     20\u001b[39m         line\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.startswith(ignore_lines_startswith)\n\u001b[32m     23\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Mestrado\\Inteligencia_Artificial\\Projeto\\Artificial-Inteligence\\.venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[39m, in \u001b[36mCorpusReader.raw\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m    216\u001b[39m contents = []\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    219\u001b[39m         contents.append(fp.read())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Mestrado\\Inteligencia_Artificial\\Projeto\\Artificial-Inteligence\\.venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[39m, in \u001b[36mCorpusReader.open\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.encoding(file)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m.open(encoding)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Mestrado\\Inteligencia_Artificial\\Projeto\\Artificial-Inteligence\\.venv\\Lib\\site-packages\\nltk\\data.py:333\u001b[39m, in \u001b[36mFileSystemPathPointer.join\u001b[39m\u001b[34m(self, fileid)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[32m    332\u001b[39m     _path = os.path.join(\u001b[38;5;28mself\u001b[39m._path, fileid)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Mestrado\\Inteligencia_Artificial\\Projeto\\Artificial-Inteligence\\.venv\\Lib\\site-packages\\nltk\\data.py:310\u001b[39m, in \u001b[36mFileSystemPathPointer.__init__\u001b[39m\u001b[34m(self, _path)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m _path = os.path.abspath(_path)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % _path)\n\u001b[32m    312\u001b[39m \u001b[38;5;28mself\u001b[39m._path = _path\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train = calculate_word_counts(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvtresnove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
